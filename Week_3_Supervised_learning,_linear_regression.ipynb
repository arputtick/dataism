{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 3: Supervised learning, linear regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO130V2nz2JNyJDVYUrzKOd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arputtick/dataism/blob/master/Week_3_Supervised_learning%2C_linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSuBT94XqUQK",
        "colab_type": "text"
      },
      "source": [
        "# **Supervised Learning**\n",
        "\n",
        "Recall the \"five steps of supervised learning\" from class:\n",
        "\n",
        "1) **Start with labeled data**: $(x_1,y_1), (x_2, y_2), ... , (x_n, y_n).$ Here $n$ is the number of training samples, the $x_i$ are the samples, and the $y_i$ are the labels.\n",
        "\n",
        "2) **Begin with an initial random model:**\n",
        "A model is a map $f$ from inputs to outputs that takes a new sample $x$ and gives it a label $y = f(x).$\n",
        "\n",
        "3) **Define a \"loss function\":**\n",
        "A number measuring how good/bad the predictions are.\n",
        "\n",
        "4) **Gradually adjust the model $f$ to decrease the loss:**\n",
        "I.e., Iteratively sdjust $f$ so that it's predictions become better and better. Do this with *gradient descent.*\n",
        "\n",
        "5) **Best model when loss is minimized.**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "volrv4iGwNxz",
        "colab_type": "text"
      },
      "source": [
        "# **Linear Regression**\n",
        "\n",
        "Let's go through the five steps in the context of *linear regression* , i.e., finding the line that best describes a 2D set of data points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bHZPHgHhWO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the necessary libraries:\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # matplotlib.pyplot contains a large variety of \n",
        "                                # visualization tools."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3hiolyexHc0",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# 1. Start with labeled data:\n",
        "\n",
        "We start with the following data consisting of CO2-emissions taxes and the corresponding amount of CO2-emissions:\n",
        "\n",
        "| Tax ($/metric ton) | CO2-Emissions (metric tons)|\n",
        "|---|---|\n",
        "|5|5|\n",
        "|10|4.3|\n",
        "|15|3.7|\n",
        "|25|2.9|\n",
        "\n",
        "The following code saves the first column into a numpy array named 'X.' \n",
        "\n",
        "These are our samples $x_1 = 5,\\, x_2 = 10, \\,x_3 = 15,\\, x_4 = 25.$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA06mh8Zhik5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.array([5,10,15,25])\n",
        "print(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L81hXaDtyxj-",
        "colab_type": "text"
      },
      "source": [
        "We save the second column unto a numpy array named 'Y.' These are the labels $y_1 = 5,\\, y_2 = 4.3,\\, \\ldots$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBnrUVxdh5up",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y = np.array([5,4.3,3.7,2.9])\n",
        "print(Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVJKp654h_m7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the labeled data\n",
        "\n",
        "# Specify that we're plotting X against Y. 'bo' stands for blue dots and\n",
        "# describes the style of the plot.\n",
        "plt.plot(X,Y, 'bo') \n",
        "\n",
        "# label the axes\n",
        "plt.ylabel('Tax')\n",
        "plt.xlabel('Co2-Emissions')\n",
        "\n",
        "# start the plot at the point (0,0)\n",
        "plt.xlim(xmin = 0)\n",
        "plt.ylim(ymin = 0)\n",
        "\n",
        "# display the plot\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYM2jJAI0kXb",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# 2. Begin with an initial random model:\n",
        "\n",
        "We start with a randomly chosen line $f(x) = wx + b.$ Random means that we choose $w$ and $b$ randomly.\n",
        "\n",
        "In class I didn't choose completely randomly. I made sure to start with a negative slope $(w<0)$ and chose $b =5.$\n",
        "\n",
        "Why? Remember that are goal is to minimize the loss or \"reaching the valley from up in the mountains.\"\n",
        "\n",
        "Actually, there might be multiple valleys. Depending on the initial random model you start with, you might end up in a different valley (see image).\n",
        "\n",
        "![alt text](https://github.com/arputtick/dataism/raw/master/assignments/Week%203/Screen%20Shot%202020-06-11%20at%2016.50.48.png)\n",
        "\n",
        "We want the loss to be as small as possible, so we have to take care to avoid the shallower valley. Therefore, it always makes sense to use some knowledge to pick a starting point closer to the correct valley. From the picture, we can see that best line should have negative slope $(w <0)$ and $b$ should be close to 5.\n",
        "\n",
        "*Exercise*: Try choosing other values of $w$ and $b$ and see if you can make sense of what happens when you do gradient descent.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dp0SM9KKid1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# randomly initialize the slope\n",
        "w = -10 * np.random.random_sample() # random number between -10 and 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK08nvWoi5gi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# not so randomly initialize the y-intercept\n",
        "b = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyu5Gx-ZjCxB",
        "colab_type": "code",
        "outputId": "a7957a04-1d06-4e19-add4-e83b72ccf6cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(w,b) # Check values of w and b"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-4.037754519376798 5.393338748942456\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHK7qOqc5GIp",
        "colab_type": "text"
      },
      "source": [
        "Let's also visualize our initial model:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr_FZApi5RS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(X, Y, 'bo')\n",
        "\n",
        "# make list of one hundred evenly spaced X-values between 0 and 30)\n",
        "x = np.linspace(0,30,100)\n",
        "\n",
        "# the corresponding list of predictions\n",
        "y = w*x + b\n",
        "\n",
        "# connect the points from these lists with a red line\n",
        "plt.plot(x, y, '-r') # -r = 'red line'\n",
        "\n",
        "plt.ylim(ymin = 0)\n",
        "plt.xlim(xmin = 0)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOiw8mOEjHLG",
        "colab_type": "text"
      },
      "source": [
        "## Initial weights\n",
        "\n",
        "The general Machine Learning term for the w and b that we start with: *initial weights.*\n",
        "\n",
        "Our choice of initial weights determine where in the mountains we start our descent from. In situations where there are many valleys (local minima), it is customary to choose several different starting points and compare the models we end up with. \n",
        "\n",
        "One of the least understood phenomena of deep neural networks is that, even though there are many, many local minima, almost all of them seem to produce good models.\n",
        "\n",
        "## Note on models vs. algorithms\n",
        "\n",
        "The words 'model' and 'algorithm' are often used interchangeably, but there is a standardized use in the context of machine learning. Usually we say that we use machine learning *algorithms* (like linear regression) to obtain a trained *model* (like the best-fit line). \n",
        "\n",
        "The confusion is that the trained model is itself an algorithm. It takes in a certain input and performs a series of steps that were \"learned\" during training and produces a desired output. Hence the idea that machine learning is about writing \"algorithms that learn algorithms.\"\n",
        "\n",
        "In our example, the input of the model is an arbitrary emissions tax level and the output is the predicted level of CO2-emissions at that tax-level.\n",
        "\n",
        "In other data science contexts, 'model' can have a completely different meaning. For instance, 'modeling the data' might mean creating a some sort of visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeoX0LJl5SLN",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 3. Define a Loss Function\n",
        "\n",
        "For linear regression (and other 'regression problems' in machine learning), a typical choice of loss function is the **mean-squared error:**\n",
        "\n",
        "$$Loss = \\frac{1}{n}\\sum_{i=1}^{n}(f(x_i)-y_i)^2,$$\n",
        "\n",
        "where $n$ is the number of training samples (in our example $n =4$) and $f(x_i) = wx_i + b.$\n",
        "\n",
        "Remember, the loss should be interpreted as the average of the (squared) distances between our data points and the current line. \n",
        "\n",
        "*Exercise:* Think about why it makes sense that the 'best line' would minimize this average.\n",
        "\n",
        "First we define an array called 'Y_predict:'\n",
        "\n",
        "$$\\mbox{Y_predict} = [f(x_1),\\, f(x_2),\\,f(x_3), \\,f(x_4)]$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_a1lJZfjRic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_predict = w*X + b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTxjKdADjiKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_predict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-U9bAE-Ieup",
        "colab_type": "text"
      },
      "source": [
        "Now we compare the predictions and the labels:\n",
        "\n",
        "$$\\mbox{diff} = [f(x_1)-y_1,\\, f(x_2)-y_2,\\,f(x_3)-y_3, \\,f(x_4)-y_4]$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nLlDXElj0JZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# difference between predictions and labels\n",
        "diff = Y_predict - Y\n",
        "diff"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofDq-MWqIsCv",
        "colab_type": "text"
      },
      "source": [
        "Then we square them:\n",
        "$$\\mbox{sq_diff} = [(f(x_1)-y_1)^2,\\, (f(x_2)-y_2)^2,\\,(f(x_3)-y_3)^2, \\,(f(x_4)-y_4)^2]$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXM7DlVJj9y5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# square differences\n",
        "sq_diff = diff**2\n",
        "sq_diff"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZvFL2m6I_Sx",
        "colab_type": "text"
      },
      "source": [
        "Finally, we average the entries in 'sq_diff' to obtain the loss function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9D2Wk4gkR0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = 1/len(X) * np.sum(sq_diff)\n",
        "\n",
        "# len(X) = 4. The len() function returns the length of an array.\n",
        "# We write len(X) instead of 4 so that our expression for the loss\n",
        "# works for any number of data points."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Dyyd_2Dkemv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C46VprSHHD9u",
        "colab_type": "text"
      },
      "source": [
        "## Different loss functions for different situations\n",
        "It's important to know that the choice of loss function in general depends on the situation. There are standard choices for regression or (image) classification, but you can also modify the loss function and get interesting results! \n",
        "\n",
        "For example, [neural style transfer](https://www.tensorflow.org/tutorials/generative/style_transfer) is based on supervised learning in which the label for an image is itself and the loss function includes two parts: \n",
        "\n",
        "1) the ordinary mean-squared error and\n",
        "\n",
        "2) a term that measures the distance between a given image and the painting who's style you want to transfer to that image. \n",
        "\n",
        "If you try to make both terms small at once, you end up with a mixed image.\n",
        "\n",
        "In the more activistic direction, there is [research](http://jmlr.org/papers/v20/18-262.html) concerning adding constraints to the loss function to combat unfairness in the resulting model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQL5rpKiJ6JM",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# 4. Gradually Adjust the Model to Decrease Loss (Gradient Descent)\n",
        "\n",
        "Now we begin descending from our initial random model into the valley where the loss is minimal.\n",
        "\n",
        "First we set the step size or 'learning rate,' which we called $\\alpha$ in the slides."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzwkftlSkgHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = .0001 # this is alpha, step size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PcpdY0HL9Ry",
        "colab_type": "text"
      },
      "source": [
        "'Taking a step in the direction of steepest descent' means updating w and b using the following formulae:\n",
        "$$w_{new} = w_{old} - \\alpha\\cdot \\frac{2}{n}\\sum_{i=1}^{n}(f(x_i)-y_i)\\cdot x_i$$\n",
        "\n",
        "$$b_{new} = b_{old} - \\alpha\\cdot \\frac{2}{n}\\sum_{i=1}^{n}(f(x_i)-y_i).$$\n",
        "\n",
        "In case you know calculus and are curious where this came from, you can also write this as:\n",
        "\n",
        "$$w_{new} = w_{old} - \\alpha\\cdot \\frac{\\partial Loss}{\\partial w}$$\n",
        "\n",
        "$$b_{new} = b_{old} - \\alpha\\cdot \\frac{\\partial Loss}{\\partial b}.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryZ0mX-HL_cW",
        "colab_type": "text"
      },
      "source": [
        "First we have to compute the terms that get multiplied by $\\alpha$ in the above formula. We name these 'dw' and 'db':\n",
        "$$dw = \\frac{2}{n}\\sum_{i=1}^{n}(f(x_i)-y_i)\\cdot x_i$$\n",
        "\n",
        "$$db = \\frac{2}{n}\\sum_{i=1}^{n}(f(x_i)-y_i).$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAQ-mJYdlEmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dw = 2/len(X) * np.sum((Y_predict - Y) * X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntrjLSS1lv5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "db = 2/len(X) * np.sum(Y_predict - Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXe-oFrwMmBg",
        "colab_type": "text"
      },
      "source": [
        "**Note:** The 'step size' isn't exactly $\\alpha$, but $\\alpha\\cdot db$ and $\\alpha\\cdot dw.$ Both $dw$ and $db$ become smaller and smaller as we near the bottom of the valley, resulting in smaller and smaller steps, as David pointed out in class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiVBYotkNQ3l",
        "colab_type": "text"
      },
      "source": [
        "Now we 'take a step,' i.e., update w and b as in the above formulae:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK890y93l7tW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w = w - learning_rate * dw"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfWT4960mMV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvuCbuvimSK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b = b - learning_rate * db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPII836XmYS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B2aRDTvNoI7",
        "colab_type": "text"
      },
      "source": [
        "**That was just a single step.** The following code repeats the whole process over and over until we reach the bottom of the valley:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hbHR7yImZiz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## First specify the number of steps you want to take\n",
        "num_steps = 500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "940wCcVemrEv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## We want to keep track of the loss (how high up in the mountains we are)\n",
        "## after every step. We do this so we can visualize who the loss changed as\n",
        "## we descended.\n",
        "\n",
        "## To do this, start with an empty array, which we call 'loss_hist' for \n",
        "## 'Loss history.' We will add the loss at our current location with every step.\n",
        "loss_hist = [] # empty array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tHV81E9mv1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## The following for-loop repeats the process of taking a step in the\n",
        "## direction of steepest descent. Each time the loop runs, it uses the\n",
        "## w and b at our current location.\n",
        "\n",
        "for step in range(num_steps): # range(num_steps) is a list [0,1,...,999]\n",
        "    Y_predict = w*X + b\n",
        "\n",
        "    # Computing the loss at our location in the mountains\n",
        "    diff = Y_predict - Y\n",
        "    sq_diff = diff**2\n",
        "    loss = 1/len(X) * np.sum(sq_diff)\n",
        "\n",
        "    # Compute the 'gradient'\n",
        "    dw = 2/len(X) * np.sum((Y_predict - Y) * X)\n",
        "    db = 2/len(X) * np.sum(Y_predict - Y)\n",
        "\n",
        "    # Take the step:\n",
        "    w = w - learning_rate * dw\n",
        "    b = b - learning_rate * db\n",
        "\n",
        "    # record the loss at every step by adding to our loss_hist array.\n",
        "    loss_hist.append(loss)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVARNuyKPft1",
        "colab_type": "text"
      },
      "source": [
        "Let's see what the values of w and b that we ended up with are:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0fCMvzeoZhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPlJhDX_obqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_qlCy1kPnD6",
        "colab_type": "text"
      },
      "source": [
        "We can also plot the line we ended up with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_uJgBxsohcB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# include the original data points in our plot\n",
        "plt.plot(X, Y, 'bo') \n",
        "\n",
        "# take a long list of 100 values on the x-axis between 0 and 30\n",
        "x = np.linspace(0,30,100)\n",
        "\n",
        "# compute the corresponding y-values for that long list\n",
        "y = w*x + b # predictions for values in that long list\n",
        "\n",
        "# connect all of these points with a red line\n",
        "plt.plot(x, y, '-r') # -r = red line\n",
        "\n",
        "# start the plot at the point (0,0)\n",
        "plt.ylim(ymin = 0)\n",
        "plt.xlim(xmin = 0)\n",
        "\n",
        "# display the plot\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cnrjG0QQq9I",
        "colab_type": "text"
      },
      "source": [
        "And visualize how the loss changed over time:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO0t7BQNpG5F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(loss_hist)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsTKXtebSeRz",
        "colab_type": "text"
      },
      "source": [
        "As was noted during class, there's a trade off between the size the steps we take and the number of steps it takes us to reach the valley. Smaller steps means we need to take more to steps to get there.\n",
        "\n",
        "But **our steps can be too big**. Imaging you have immensely long legs and step so far that you reach a point on the other side of the valley that's even higher up from where you started! If you do this over and over, you'll end up climbing to infinity.\n",
        "\n",
        "If you really want to optimize and reach the valley as few steps as possible, you should make $\\alpha$ as big as you can without ending up in this climbing to infinty situation.\n",
        "\n",
        "*Exercise:* Play around with different values of the 'learning rate' $\\alpha$ and the number of steps 'num_steps.' Try to find the best values i.e. largest $\\alpha$ that works and the smallest number of steps.\n",
        "\n",
        "**Note:** As Aleks pointed out, the best thing to do would be to take huge steps in the beginning while you're still high up and gradually decrease $\\alpha$ as you get close to the bottom. There is a bunch of research about how to do that. You can google \"adaptive learning rates.\""
      ]
    }
  ]
}